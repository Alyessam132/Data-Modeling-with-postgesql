# Sparkify Data Modeling with Postgresql

## Purpose of the database

- A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. 
- The analytics team is particularly interested in understanding what songs users are listening to. 
- Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

## Data used

### Song File

Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID

And below is an example of what a single song file looks like

{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

### log file

- The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

- The log files in the dataset are partitioned by year and month.

![Alt Text](log-data.png)

## Solution

-  Make ETL Pipeline to extract the song and log files
-  Create a Postgres database with tables designed to optimize queries on song play analysis 
-  Test the database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results.

## Database Schema

- The database schema is star schema to implement queries faster as below
    - The Fact Table is songplays table
    - The dimension tables are (users, songs, artists and time)

![Alt text](database-schema.png)

## ETL Pipeline

After creating the database the ETL pipeline consists of:

- Converting the song files and log files to dataframes using pandas
- inserting the data from the song files dataframe into the **songs and artists** tables
- Converting the log file column (ts) into timestamp
- Making the different time formats (hour, week, day, etc) and inserting them into **time** table
- Inserting the data from the log files dataframe into the **songplays and users** tables

## Repo Files

- data folder : contains the song and log files
- sql_queries.py : contains the sql queries used for creation and insertion to the tables
- create_tables.py : contains the steps to create the database and run function that will create the tables in the database
- etl.ipynb: contains the steps of the etl on a simple scale by using 1 song file and 1 log file
- etl.py: conatins the steps of the etl but it will extract all song and log files
- test.ipynb : contains the test of creation and insertion of the database

## How to run

1. run create_tables.py
2. run etl.py
3. run test.ipynb for testing
