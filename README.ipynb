{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Data Modeling with Postgresql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. \n",
    "- The analytics team is particularly interested in understanding what songs users are listening to. \n",
    "- Currently, they don't have an easy way to query their data, which resides in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Song File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID\n",
    "\n",
    "And below is an example of what a single song file looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"num_songs\": 1, \"artist_id\": \"ARJIE2Y1187B994AB7\", \"artist_latitude\": null, \"artist_longitude\": null, \"artist_location\": \"\", \"artist_name\": \"Line Renaud\", \"song_id\": \"SOUPIRU12A6D4FA1E1\", \"title\": \"Der Kleine Dompfaff\", \"duration\": 152.92036, \"year\": 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### log file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.\n",
    "\n",
    "- The log files in the dataset are partitioned by year and month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt Text](.\\log-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Make ETL Pipeline to extract the song and log files\n",
    "-  Create a Postgres database with tables designed to optimize queries on song play analysis \n",
    "-  Test the database and ETL pipeline by running queries given to you by the analytics team from Sparkify and compare your results with their expected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The database schema is star schema to implement queries faster as below\n",
    "    - The Fact Table is songplays table\n",
    "    - The dimension tables are (users, songs, artists and time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](.\\database-schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the database the ETL pipeline consists of:\n",
    "\n",
    "- Converting the song files and log files to dataframes using pandas\n",
    "- inserting the data from the song files dataframe into the **songs and artists** tables\n",
    "- Converting the log file column (ts) into timestamp\n",
    "- Making the different time formats (hour, week, day, etc) and inserting them into **time** table\n",
    "- Inserting the data from the log files dataframe into the **songplays and users** tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repo Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- data folder : contains the song and log files\n",
    "- sql_queries.py : contains the sql queries used for creation and insertion to the tables\n",
    "- create_tables.py : contains the steps to create the database and run function that will create the tables in the database\n",
    "- etl.ipynb: contains the steps of the etl on a simple scale by using 1 song file and 1 log file\n",
    "- etl.py: conatins the steps of the etl but it will extract all song and log files\n",
    "- test.ipynb : contains the test of creation and insertion of the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. run create_tables.py\n",
    "2. run etl.py\n",
    "3. run test.ipynb for testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
